---
title: Buy The Dip
author: Fennon W.
date: '2019-11-24'
slug: buy-the-dip
categories:
  - Finance
tags:
  - r
  - alpha vantage
disableComments: no
---


Warren Buffett once said that as an investor, it is wise to be “Fearful when others are greedy and greedy when others are fearful.”


In a up trend on any timescale, a smart trader would "buy the dip" -- buy any time there's a sizeable selloff. Let's define an "up trend" in the broadest sense: if price is making higher **all time highs**, it's in an uptrend. So any time a new all time high is achieved, and price starts to slide, does buying into the selloff and fear produce positive returns?


We'll test this using **SPY**, the SPDR S&P500 ETF designed to track the S&P 500 stock market index. This ETF is very liquid, covers a broad range of stocks (so no cherry picking), and can be traded commission free by most brokerages. **However, trade at your own risk.** Neither I, nor your parents, nor Jesus are responsible for your financial decisions. 

To obtain historical ETF data, I used the <a href="https://github.com/business-science/alphavantager">alphavantager</a> R package to access the <a href="https://www.alphavantage.co/documentation/">Alpha Vantage API</a>; the <a href="https://github.com/business-science/tidyquant">tidyquant</a> package to calculate simple moving averages; and tidyverse, ggplot2, etc to wrangle and visualize data.

Price data covers approximately 20 years, from 1999 to 2019.


# Setup

Load libraries.

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(tidyquant)
library(reshape2)
library(ggplot2)
library(gridExtra)
library(lubridate)
library(alphavantager)
library(scales)
library(knitr)
library(kableExtra)
```

```
library(tidyverse)
library(tidyquant)
library(reshape2)
library(ggplot2)
library(lubridate)
library(alphavantager)
library(scales)
```

Set Alphavantager key.
```{r eval = FALSE}
av_api_key("my_key_goes_here")
```

```{r echo = FALSE}
av_api_key("ZVWHBIOOHGCNKHD3")
```

# Get Historical ETF Data

Get all available price data.

```{r echo = FALSE, eval = FALSE, message = FALSE}
av_get(symbol = "SPY"
       , av_fun = "TIME_SERIES_DAILY_ADJUSTED"
       , outputsize = "full") %>%
  mutate(row_id = row_number()) %>%
  select(row_id, timestamp, open, close) -> all_dat
```

```{r echo = FALSE}
all_dat <- readRDS("/Users/Fennon/Desktop/My Blog/projects/buy_the_dip/all_dat_fat.rds")
```

Let's take a look at ATHs over time.
```{r, echo = FALSE, message = FALSE}
## calc all time highs (ath) + difference between ath
all_dat %>% 
  mutate(row_id = row_number()
         , ath = cummax(high)) %>% 
  group_by() %>% 
  mutate(diff_ath = ath - lag(ath)) %>% 
  ungroup() %>% 
  filter(diff_ath != 0) %>% 
  mutate(diff_ts = as.numeric(timestamp - lag(timestamp))) -> all_dat_ath
```

```{r warning = FALSE, message = FALSE}
## plot all time highs
all_dat %>% 
  ggplot(aes(x = timestamp, y = close)) +
  geom_line() +
  geom_point(data = all_dat_ath, aes(x = timestamp, y = close), color = "green") +
  xlab("Date") +
  ylab("Close") +
  theme_classic() +
  ggtitle("SPY All Time Highs") -> plot_ath

plot_ath
```

The second half of the dataset has significantly more ATHs than the first. We would have prefered a more balanced universe over time, but oh well. 

We'll calculate all time highs (ATH) and the time difference between consecutive ones. Let's assume that a "dip" must consist of at least three days: all time high, min, next all time high. Therefore, we'll filter out back-to-back days of ATHs.

```{r}
all_dat %>% 
  mutate(row_id = row_number()
         , ath = cummax(high)) %>% 
  group_by() %>% 
  mutate(diff_ath = ath - lag(ath)) %>% 
  ungroup() %>% 
  filter(diff_ath != 0) %>% 
  mutate(diff_ts = as.numeric(timestamp - lag(timestamp))) -> all_dat_ath
```

Now, I am sure there is an easier way to calculate drawdowns between two known points (like using the PerformanceAnalytics package), but who doesn't like to make their lives unnecessarily more challenging? So we'll calculate drawdowns as follows:

1. Get dates of consecutive ATH pairs.
2. Filter original price dataset to contain all dates between those from Step 1.
3. Calculate minimum price between ATH pairs.
4. Subtract minimum from first ATH.

```{r}
## loop through consecutive pairs of ath_ind
drawdown_dat <- data.frame(row_id = as.numeric()
                           , drawdown = as.numeric()
                           , drawndown_dt = as.character())

for(i in 1:(nrow(all_dat_ath)-1)){
  
  # get timestamps of consecutive ath
  ath1_ts <- all_dat_ath[i, ]$timestamp
  ath2_ts <- all_dat_ath[i + 1, ]$timestamp
  
  # calc min between timestamps
  all_dat %>% 
    filter(timestamp >= ath1_ts, timestamp <= ath2_ts) -> intra_peak_price
  
  # only aths with more than 1-day in between
  if(nrow(intra_peak_price) > 2){
    
    ath1_line <- head(intra_peak_price, 1)
    ath2_line <- tail(intra_peak_price, 1)
    dd_line <- intra_peak_price[intra_peak_price$low == min(intra_peak_price$low), ]
    
    intra_peak_price %>% 
      group_by() %>% 
      summarise(prior_peak = ath1_line$high
                , valley = min(low)
                , drawdown = ((valley - prior_peak)) / prior_peak) %>% 
      ungroup() -> drawdown_tmp 
    
    drawdown_tmp %>% 
      inner_join(dd_line, by = c("valley" = "low")) %>% 
      rename("drawdown_dt" = "timestamp") %>% 
      select(row_id, drawdown_dt, drawdown) -> drawdown_dat_add
    
    drawdown_dat_add %>% 
      rbind.data.frame(drawdown_dat) -> drawdown_dat
    
    ## Doesn't non-consistent arrow direction drive you crazy?
  }
}
```

With all our drawdowns calculated, let's break those down into the following categories:

* -1% to -5%
* -5% to -10%
* -10% to -20%
* < -20%

Each category is plotted below, with the max drawdown of -57% occuring during the financial crisis in 2008. Scary times...

```{r echo = FALSE}
drawdown_dat %>% 
  mutate(drawdown = as.numeric(unlist(drawdown))) %>% 
  mutate(dd_thresh = case_when (drawdown <= -0.01 & drawdown > -0.05 ~ "-1% to -5%"
                                , drawdown <= -0.05 & drawdown > -0.10 ~ "-5% to -10%"
                                , drawdown <= -0.1 & drawdown > -0.20 ~ "-10% to -20%"
                                , drawdown <= -0.2 ~ "< -20%")) %>% 
  filter(!is.na(dd_thresh)) -> drawdown_cat

all_dat %>%
  ggplot(aes(x = timestamp, y = close)) +
  geom_line() +
  geom_vline(data = drawdown_cat %>% filter(dd_thresh == "-1% to -5%")
             , aes(xintercept = drawdown_dt), color = "green") +
  xlab("Date") +
  ylab("Close") +
  theme_classic() +
  ggtitle("-1% to -5%") -> plot_thresh1

all_dat %>% 
  ggplot(aes(x = timestamp, y = close)) +
  geom_line() +
  geom_vline(data = drawdown_cat %>% filter(dd_thresh == "-5% to -10%")
             , aes(xintercept = drawdown_dt), color = "blue") +
  xlab("Date") +
  ylab("Close") +
  theme_classic() +
  ggtitle("-5% to -10%") -> plot_thresh2

all_dat %>% 
  ggplot(aes(x = timestamp, y = close)) +
  geom_line() +
  geom_vline(data = drawdown_cat %>% filter(dd_thresh == "-10% to -20%")
             , aes(xintercept = drawdown_dt), color = "orange") +
  xlab("Date") +
  ylab("Close") +
  theme_classic() +
  ggtitle("-10% to -20%") -> plot_thresh3

all_dat %>% 
  ggplot(aes(x = timestamp, y = close)) +
  geom_line() +
  geom_vline(data = drawdown_cat %>% filter(dd_thresh == "< -20%")
             , aes(xintercept = drawdown_dt), color = "red") +
  xlab("Date") +
  ylab("Close") +
  theme_classic() +
  ggtitle("< -20%") -> plot_thresh4

```

```{r echo = FALSE, fig.width = 8.5, fig.height = 6}
grid.arrange(plot_thresh1, plot_thresh2, plot_thresh3, plot_thresh4)
```

What would've happened if we bought in during those fear-filled drawdowns? Below are the returns over the next 20, 50, 100, and 200 days after entering each drawdown category (dd_cat) :

```{r echo = FALSE, message = FALSE, warning = FALSE}
## for each dip category, how much return 20, 50, 100, 200 days out
dip_performance = data.frame(dd_cat = as.character()
                             , d20 = as.numeric()
                             , d50 = as.numeric()
                             , d100 = as.numeric()
                             , d200 = as.numeric())

for(i in 1:nrow(drawdown_cat)){
  
  # all dates after drawdown
  all_dat %>%  
    filter(timestamp >= drawdown_cat$drawdown_dt[i]) %>% 
    mutate(row_id2 = row_number()
           , days_forward = case_when(row_id2 == 1 ~ "start",
                                      row_id2 == 20 ~ "d20",
                                      row_id2 == 50 ~ "d50",
                                      row_id2 == 100 ~ "d100",
                                      row_id2 == 200 ~ "d200")) %>% 
    filter(!is.na(days_forward)) %>% 
    select(close, days_forward) %>% 
    spread(days_forward, close) %>% 
    mutate_at(vars(-start), funs((./start)-1)) %>% 
    mutate(dd_cat = drawdown_cat$dd_thresh[i]) %>% 
    select(-start) -> perf_tmp
  
  dip_performance %>% 
    bind_rows(perf_tmp) -> dip_performance
  
}

## summarize results
dip_performance %>% 
  group_by(dd_cat) %>% 
  summarise(n = n()
            , d20 = scales::percent(mean(d20, na.rm = TRUE))
            , d50 = scales::percent(mean(d50, na.rm = TRUE))
            , d100 = scales::percent(mean(d100, na.rm = TRUE))
            , d200 = scales::percent(mean(d200, na.rm = TRUE))) %>% 
  ungroup() %>% 
  as.data.frame() -> dip_performance_sum

knitr::kable(dip_performance_sum)
```

Surprisingly, buying in when others are fearful produces positive returns over the short-to-long term, with the largest return actually coming off the largest drawdowns.

Obviously we can't know a drawdown falls into one of those categories until the next ATH is in place. And it would definitely be difficult to get in at a "3% correction" only for prices to sell off an additional 50%. But the take away is that "buying the dip", buying when others are fearful, *can* produce positive results over time. 

Maybe Warren Buffet knows what he's talking about...
